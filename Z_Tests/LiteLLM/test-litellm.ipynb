{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f17650fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649fc7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: litellm in /home/jalcocert/.local/lib/python3.10/site-packages (1.68.1)\n",
      "Requirement already satisfied: aiohttp in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (3.8.4)\n",
      "Requirement already satisfied: tokenizers in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (0.13.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (2.11.4)\n",
      "Requirement already satisfied: openai<1.76.0,>=1.68.2 in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (1.75.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (4.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (1.0.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (3.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (0.9.0)\n",
      "Requirement already satisfied: click in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (8.1.3)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (0.23.3)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from litellm) (8.7.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /home/jalcocert/.local/lib/python3.10/site-packages (from httpx>=0.23.0->litellm) (1.5.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from httpx>=0.23.0->litellm) (0.16.3)\n",
      "Requirement already satisfied: sniffio in /home/jalcocert/.local/lib/python3.10/site-packages (from httpx>=0.23.0->litellm) (1.3.0)\n",
      "Requirement already satisfied: certifi in /home/jalcocert/.local/lib/python3.10/site-packages (from httpx>=0.23.0->litellm) (2022.12.7)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/jalcocert/.local/lib/python3.10/site-packages (from importlib-metadata>=6.8.0->litellm) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/jalcocert/.local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.4.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (22.2.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/jalcocert/.local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/jalcocert/.local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.24.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from openai<1.76.0,>=1.68.2->litellm) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from openai<1.76.0,>=1.68.2->litellm) (3.6.2)\n",
      "Requirement already satisfied: tqdm>4 in /home/jalcocert/.local/lib/python3.10/site-packages (from openai<1.76.0,>=1.68.2->litellm) (4.65.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<1.76.0,>=1.68.2->litellm) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/jalcocert/.local/lib/python3.10/site-packages (from openai<1.76.0,>=1.68.2->litellm) (4.13.2)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/jalcocert/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/jalcocert/.local/lib/python3.10/site-packages (from tiktoken>=0.7.0->litellm) (2023.3.23)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from tiktoken>=0.7.0->litellm) (2.28.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jalcocert/.local/lib/python3.10/site-packages (from aiohttp->litellm) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jalcocert/.local/lib/python3.10/site-packages (from aiohttp->litellm) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jalcocert/.local/lib/python3.10/site-packages (from aiohttp->litellm) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from aiohttp->litellm) (3.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jalcocert/.local/lib/python3.10/site-packages (from aiohttp->litellm) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jalcocert/.local/lib/python3.10/site-packages (from aiohttp->litellm) (1.3.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/jalcocert/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<1.76.0,>=1.68.2->litellm) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/jalcocert/.local/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx>=0.23.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jalcocert/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (1.26.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b09bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: litellm\n",
      "Version: 1.68.1\n",
      "Summary: Library to easily interface with LLM API providers\n",
      "Home-page: \n",
      "Author: BerriAI\n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/jalcocert/.local/lib/python3.10/site-packages\n",
      "Requires: aiohttp, click, httpx, importlib-metadata, jinja2, jsonschema, openai, pydantic, python-dotenv, tiktoken, tokenizers\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd58be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPENAI\n",
    "\n",
    "# !export OPENAI_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bd9c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72196122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## set ENV variables\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed447a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e58bea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Response: ModelResponse(id='chatcmpl-BUZ1gIeLRemSd4nqBZydUKinoFRz7', created=1746624180, model='gpt-4.1-nano-2025-04-14', object='chat.completion', system_fingerprint='fp_8fd43718b3', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm doing well, thank you. How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=17, prompt_tokens=13, total_tokens=30, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "AI Response: Hello! I'm doing well, thank you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key from the environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(openai_api_key)\n",
    "\n",
    "# Check if the API key is loaded\n",
    "if openai_api_key:\n",
    "    try:\n",
    "        response = completion(\n",
    "          model=\"openai/gpt-4.1-nano\",\n",
    "          messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    "          api_key=openai_api_key  # Explicitly pass the API key\n",
    "        )\n",
    "        print(\"Raw Response:\", response)  # To see the full structure\n",
    "\n",
    "        if response.choices:\n",
    "            first_choice = response.choices[0]\n",
    "            if first_choice.message:\n",
    "                ai_response = first_choice.message.content\n",
    "                print(\"AI Response:\", ai_response)\n",
    "            else:\n",
    "                print(\"Error: First choice does not contain a message.\")\n",
    "        else:\n",
    "            print(\"Error: No choices found in the response.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: OPENAI_API_KEY not found in environment variables or .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2ef486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Response: ModelResponse(id='chatcmpl-d33148d6-3345-44f8-8c7d-0ac9d2b48117', created=1746624354, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_dadc9d6142', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello there Litellm! It's great to meet you! Is there something I can help you with, or would you like to chat about something in particular? I'm here to listen and assist if I can.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=45, prompt_tokens=15, total_tokens=60, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.162854996, prompt_time=0.002441786, completion_time=0.0375, total_time=0.039941786), usage_breakdown={'models': None}, x_groq={'id': 'req_01jtnes63kexa8q794mg1pcvks'})\n",
      "AI Response: Hello there Litellm! It's great to meet you! Is there something I can help you with, or would you like to chat about something in particular? I'm here to listen and assist if I can.\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Groq API key from environment variables\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if groq_api_key:\n",
    "    try:\n",
    "        response = completion(\n",
    "            model=\"groq/llama3-8b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"hello from litellm\"}],\n",
    "            api_key=groq_api_key  # Explicitly pass the Groq API key\n",
    "        )\n",
    "        print(\"Raw Response:\", response)  # To see the full structure\n",
    "\n",
    "        if response.choices:\n",
    "            first_choice = response.choices[0]\n",
    "            if first_choice.message:\n",
    "                ai_response = first_choice.message.content\n",
    "                print(\"AI Response:\", ai_response)\n",
    "            else:\n",
    "                print(\"Error: First choice does not contain a message.\")\n",
    "        else:\n",
    "            print(\"Error: No choices found in the response.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: GROQ_API_KEY not found in environment variables or .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb9310e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21921c7",
   "metadata": {},
   "source": [
    "* Setup Ollama\n",
    "* See [available models](https://github.com/JAlcocerT/Streamlit-MultiChat/blob/main/Z_Tests/Z_API_Models_check.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "586fb419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-6f7972de-6ebc-4542-82e2-0c29b1ab960c', created=1746623975, model='ollama/llama3.2:latest', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I'm just a language model, so I don't have feelings or emotions like humans do. But thank you for asking! How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=33, prompt_tokens=34, total_tokens=67, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "            model=\"ollama/llama3.2:latest\",\n",
    "            messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    "            #api_base=\"http://localhost:11434\",\n",
    "            api_base=\"http://192.168.1.11:11434\",\n",
    "            stream=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "591a6d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Response: ModelResponse(id='chatcmpl-c9f5546f-e5e6-4b03-87db-487d4ddbc64c', created=1746624022, model='ollama/llama3.2:latest', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm here and ready to help you with any questions or tasks you may have! How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=46, prompt_tokens=34, total_tokens=80, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "AI Response: I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm here and ready to help you with any questions or tasks you may have! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Ollama base URL (if set)\n",
    "ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\", \"http://192.168.1.11:11434\")\n",
    "\n",
    "try:\n",
    "    response = completion(\n",
    "        model=\"ollama/llama3.2:latest\",\n",
    "        messages=[{\"content\": \"Hello, how are you?\", \"role\": \"user\"}],\n",
    "        base_url=ollama_base_url  # Specify the base URL for Ollama\n",
    "    )\n",
    "\n",
    "    # Print the raw response for inspection (optional)\n",
    "    print(\"Raw Response:\", response)\n",
    "\n",
    "    # Check if there are any choices in the response\n",
    "    if response.choices:\n",
    "        # Get the first choice (usually there's only one for a simple text completion)\n",
    "        first_choice = response.choices[0]\n",
    "\n",
    "        # Check if the choice has a message\n",
    "        if first_choice.message:\n",
    "            # Access the content of the message\n",
    "            ai_response = first_choice.message.content\n",
    "            print(\"AI Response:\", ai_response)\n",
    "        else:\n",
    "            print(\"Error: First choice in response does not contain a message.\")\n",
    "    else:\n",
    "        print(\"Error: No choices found in the response.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the API call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HF\n",
    "#https://huggingface.co/\n",
    "#https://ui.endpoints.huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "323fa381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: HUGGINGFACE_API_KEY or api_base not properly set.\n",
      "Please ensure you have set the HUGGINGFACE_API_KEY environment variable\n",
      "and replaced 'YOUR_HUGGINGFACE_ENDPOINT_URL' with your actual endpoint.\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Hugging Face API key from environment variables\n",
    "huggingface_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# Define the Hugging Face Inference Endpoint base URL\n",
    "huggingface_api_base = \"YOUR_HUGGINGFACE_ENDPOINT_URL\"  # Replace with your actual endpoint\n",
    "\n",
    "if huggingface_api_key and huggingface_api_base != \"YOUR_HUGGINGFACE_ENDPOINT_URL\":\n",
    "    try:\n",
    "        response = completion(\n",
    "          model=\"huggingface/WizardLM/WizardCoder-Python-34B-V1.0\",\n",
    "          messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    "          api_key=huggingface_api_key,\n",
    "          api_base=huggingface_api_base\n",
    "        )\n",
    "        print(\"Raw Response:\", response)  # To see the full structure\n",
    "\n",
    "        if response.choices:\n",
    "            first_choice = response.choices[0]\n",
    "            if first_choice.message:\n",
    "                ai_response = first_choice.message.content\n",
    "                print(\"AI Response:\", ai_response)\n",
    "            else:\n",
    "                print(\"Error: First choice does not contain a message.\")\n",
    "        else:\n",
    "            print(\"Error: No choices found in the response.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: HUGGINGFACE_API_KEY or api_base not properly set.\")\n",
    "    print(\"Please ensure you have set the HUGGINGFACE_API_KEY environment variable\")\n",
    "    print(\"and replaced 'YOUR_HUGGINGFACE_ENDPOINT_URL' with your actual endpoint.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
